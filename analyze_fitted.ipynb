{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our notebooks run on ZIP files, which you can create using this script via Wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from ast import arg\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from itertools import product\n",
    "\n",
    "from collector import RunCollector, CompleteRunCollector\n",
    "from constants import *\n",
    "from defaults import *\n",
    "from run_configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label your Wandb runs using tags, and include the relevant ones.\n",
    "\n",
    "This saves an `npy` to the `collector_path` directory. If this file does not\n",
    "already exist, this fetches runs from Wandb. Otherwise, it just loads from the file.\n",
    "\"\"\"\n",
    "\n",
    "collector_path = 'sac_utd_bs_lr_250121_complete'\n",
    "zip_path = 'gym_250121_remake'\n",
    "\n",
    "sac_utd_bs_lr_collector = CompleteRunCollector.create(\n",
    "    project='prestonfu/crl',\n",
    "    load=True,\n",
    "    max_steps=max_steps,\n",
    "    tags=[\n",
    "        'sac_utd_bs_lr_250109', \n",
    "        'sac_utd_bs_lr_retry_250111', \n",
    "        'sac_utd_bs_lr_retry_250112', \n",
    "        'sac_utd_bs_lr_retry_250114', \n",
    "        'sac_utd_bs_lr_retry_250115', \n",
    "        'sac_utd_bs_lr_retry_250116',  \n",
    "        'sac_utd_bs_lr_250118', \n",
    "        'sac_utd_bs_lr_cheetah_250121'\n",
    "    ],\n",
    "    path=f'../output/{collector_path}/data.npy',\n",
    "    parallel=32\n",
    ")\n",
    "\n",
    "collector = sac_utd_bs_lr_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop runs that do not go for the full number of steps, and extra runs beyond \n",
    "desired number of seeds.\n",
    "\"\"\"\n",
    "\n",
    "num_seeds = 8\n",
    "collector.remove_short(0.95)\n",
    "collector.trim(num_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from copy import deepcopy\n",
    "\n",
    "def get_data(collector: CompleteRunCollector, env_name, varname, utd):\n",
    "    all_data = collector.filter(env=env_name, utd=utd)\n",
    "    data_dict = {}\n",
    "    for key, summaries in all_data.items():\n",
    "        bs = key[collector.category_index['bs']]\n",
    "        lr = key[collector.category_index['lr']]\n",
    "        name = (env_name, utd, bs, lr)\n",
    "        short_summaries = []\n",
    "        for i, df in enumerate(summaries):\n",
    "            short_summaries.append(df[['_step', varname]].rename(columns={varname: f'seed{i}/{varname}'}))\n",
    "        merged_df = reduce(\n",
    "            lambda l, r: pd.merge(l, r, on='_step', how='outer'),\n",
    "            short_summaries\n",
    "        )\n",
    "        \n",
    "        merged_df['rounded_step'] = (merged_df['_step'] // 1000) * 1000  # resolve non-uniform logging\n",
    "        agg_dict = {\n",
    "            '_step': 'first', \n",
    "            **{col: 'mean' for col in merged_df.columns if col.startswith('seed')}\n",
    "        }\n",
    "        result_df = merged_df.groupby('rounded_step').agg(agg_dict).dropna().reset_index(drop=True)\n",
    "        \n",
    "        data_dict[name] = result_df.to_numpy()\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def save_data(collector, env_name, varname, utd):\n",
    "    dirname = f'data/{zip_path}/utd_{utd}/{env_name}/{varname.replace(\"/\", \".\")}'\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    data_dict = get_data(collector, env_name, varname, utd)\n",
    "    for key, value in data_dict.items():\n",
    "        env_name, utd, bs, lr = key\n",
    "        name = f'bs_{bs}_lr_{lr}'\n",
    "        np.save(f'{dirname}/{name}', value)  \n",
    "\n",
    "def save_loop(collector):\n",
    "    for env in collector.get_unique('env'):\n",
    "        for utd in collector.get_unique('utd', env=env):\n",
    "            for varname in ['episode/return', 'val/critic_loss', 'training/critic_pnorm_l2']:\n",
    "                save_data(collector, env, varname, utd)\n",
    "\n",
    "save_loop(collector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
